{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同じデータに異なるラベルがついている場合の学習結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, n_class):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(100, 100)\n",
    "        self.fc2 = nn.Linear(100, n_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2クラス，ラベル比率が異なる場合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, loss 0.7174391150474548\n",
      "iter 10, loss 0.692090630531311\n",
      "iter 20, loss 0.6812912821769714\n",
      "iter 30, loss 0.6768698692321777\n",
      "iter 40, loss 0.6748985052108765\n",
      "predict_prob: tensor([0.6582, 0.6582, 0.6582, 0.6582, 0.6582])\n",
      "predict_class tensor([0, 0, 0, 0, 0])\n",
      "iter 0, loss 0.676060140132904\n",
      "iter 10, loss 0.6743651628494263\n",
      "iter 20, loss 0.6736418604850769\n",
      "iter 30, loss 0.6733136177062988\n",
      "iter 40, loss 0.6731592416763306\n",
      "predict_prob: tensor([0.6904, 0.6904, 0.6904, 0.6904, 0.6904])\n",
      "predict_class tensor([0, 0, 0, 0, 0])\n",
      "iter 0, loss 0.7033635973930359\n",
      "iter 10, loss 0.6848728656768799\n",
      "iter 20, loss 0.6778452396392822\n",
      "iter 30, loss 0.6751038432121277\n",
      "iter 40, loss 0.673962414264679\n",
      "predict_prob: tensor([0.6719, 0.6719, 0.6719, 0.6719, 0.6719])\n",
      "predict_class tensor([0, 0, 0, 0, 0])\n",
      "iter 0, loss 0.6820467114448547\n",
      "iter 10, loss 0.6772469878196716\n",
      "iter 20, loss 0.6751075983047485\n",
      "iter 30, loss 0.6740862131118774\n",
      "iter 40, loss 0.6735774874687195\n",
      "predict_prob: tensor([0.6776, 0.6776, 0.6776, 0.6776, 0.6776])\n",
      "predict_class tensor([0, 0, 0, 0, 0])\n",
      "iter 0, loss 0.6869224309921265\n",
      "iter 10, loss 0.6812442541122437\n",
      "iter 20, loss 0.6779704689979553\n",
      "iter 30, loss 0.6760605573654175\n",
      "iter 40, loss 0.6749314069747925\n",
      "predict_prob: tensor([0.6524, 0.6524, 0.6524, 0.6524, 0.6524])\n",
      "predict_class tensor([0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "for experiment in range(5):\n",
    "    model = Net(n_class=2)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "    data = torch.randn(1, 100).repeat(5, 1)\n",
    "    label = torch.LongTensor([0, 0, 0, 1, 1])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for iteration in range(50):\n",
    "        predict = F.softmax(model(data), dim=1)\n",
    "        loss = criterion(predict, label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if iteration % 10 == 0:\n",
    "            print('iter {}, loss {}'.format(iteration, loss))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predict_prob, predict_class = F.softmax(model(data), dim=1).max(dim=1)\n",
    "        print('predict_prob:', predict_prob)\n",
    "        print('predict_class', predict_class)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2クラス，ラベル比率が等しい場合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, loss 0.6974734663963318\n",
      "iter 10, loss 0.695296049118042\n",
      "iter 20, loss 0.6941973567008972\n",
      "iter 30, loss 0.6936560273170471\n",
      "iter 40, loss 0.6933932900428772\n",
      "predict_prob: tensor([0.5155, 0.5155, 0.5155, 0.5155, 0.5155, 0.5155])\n",
      "predict_class tensor([0, 0, 0, 0, 0, 0])\n",
      "iter 0, loss 0.6948010921478271\n",
      "iter 10, loss 0.6938408017158508\n",
      "iter 20, loss 0.6934356689453125\n",
      "iter 30, loss 0.6932666897773743\n",
      "iter 40, loss 0.6931965947151184\n",
      "predict_prob: tensor([0.5064, 0.5064, 0.5064, 0.5064, 0.5064, 0.5064])\n",
      "predict_class tensor([0, 0, 0, 0, 0, 0])\n",
      "iter 0, loss 0.6959786415100098\n",
      "iter 10, loss 0.6944865584373474\n",
      "iter 20, loss 0.6937799453735352\n",
      "iter 30, loss 0.693446934223175\n",
      "iter 40, loss 0.6932888031005859\n",
      "predict_prob: tensor([0.5116, 0.5116, 0.5116, 0.5116, 0.5116, 0.5116])\n",
      "predict_class tensor([1, 1, 1, 1, 1, 1])\n",
      "iter 0, loss 0.6942178606987\n",
      "iter 10, loss 0.6935774683952332\n",
      "iter 20, loss 0.6933190226554871\n",
      "iter 30, loss 0.693215548992157\n",
      "iter 40, loss 0.693174421787262\n",
      "predict_prob: tensor([0.5047, 0.5047, 0.5047, 0.5047, 0.5047, 0.5047])\n",
      "predict_class tensor([0, 0, 0, 0, 0, 0])\n",
      "iter 0, loss 0.699481725692749\n",
      "iter 10, loss 0.6959081292152405\n",
      "iter 20, loss 0.6943138241767883\n",
      "iter 30, loss 0.6936336159706116\n",
      "iter 40, loss 0.6933488845825195\n",
      "predict_prob: tensor([0.5129, 0.5129, 0.5129, 0.5129, 0.5129, 0.5129])\n",
      "predict_class tensor([1, 1, 1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "for experiment in range(5):\n",
    "    model = Net(n_class=2)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "    data = torch.randn(1, 100).repeat(6, 1)\n",
    "    label = torch.LongTensor([0, 0, 0, 1, 1, 1])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for iteration in range(50):\n",
    "        predict = F.softmax(model(data), dim=1)\n",
    "        loss = criterion(predict, label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if iteration % 10 == 0:\n",
    "            print('iter {}, loss {}'.format(iteration, loss))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predict_prob, predict_class = F.softmax(model(data), dim=1).max(dim=1)\n",
    "        print('predict_prob:', predict_prob)\n",
    "        print('predict_class', predict_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
